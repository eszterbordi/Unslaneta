\begin{frame}{A statisztikus gépi fordítás alapjai}
  \begin{figure}[t]
  	\includegraphics[width=1\linewidth]{images/smt}
  \end{figure}
\end{frame}

\begin{frame}{A statisztikus gépi fordítás alapjai}
  \begin{figure}[t]
  	\includegraphics[width=1\linewidth]{images/smt2}
  \end{figure}
  
  \begin{itemize}
  	\item $P(e)$ nyelvi modell: n-gram modell
  	\item $P(f|e)$ fordítási modell: IBM modell
  	\item dekóder: StackDecoder
  \end{itemize}
\end{frame}

\begin{frame}{A nyelvi modell: bigrammok}
	\begin{itemize}
		\item Adva van egy angol szósorozat: $W = w_1,w_2,...,w_n$
		\item Lánc-szabály szerint: \begin{equation*}
		\begin{split}
		p(W) = p(w_1,w_2,...,w_n) = p(w_1) \cdot p(w_2|w_1) \cdot \\p(w_3|w_1, w_2) \cdot ... \cdot p(w_n|w_1,...,w_{n-1})
		\end{split}
		\end{equation*}
		\item \textbf{Markov-feltétel} bevezetése: a múlt legutolsó lépése számít
		\begin{equation*}
		\begin{split}
		p(w_1,w_2,...,w_n) = p(w_1) \cdot p(w_2|w_1) \cdot \\p(w_3|w_2) \cdot ... \cdot p(w_n|w_{n-1})
		\end{split}
		\end{equation*}
		\item ML-becslés: 
		\begin{equation*}
		p(w_2|w_1) = \frac{count(w_1,w_2)}{count(w_1)}
		\end{equation*}
	\end{itemize}
\end{frame}

\begin{frame}{IBM1 modell}
	\begin{itemize}
		\item \textbf{Kérdés}: Hogyan modellezük a $P(f|e)$ valószínűséget?
			\begin{equation*}
			\begin{split}
			f = f_1f_2...f_m\\
			e = e_1e_2...e_l
			\end{split}
			\end{equation*}
		\item m-et rögzítve kapjuk: $p(f_1f_2...f_m|e_1e_2...e_l, m)$
		\item Hozzávesszük még a célmondat szavainak lehetséges elrendezéseit:
		$p(f_1f_2...f_m, a_1...a_m|e_1e_2...e_l, m)$
		\item \textbf{Példa:}\\
				f = Le programme a ete mis en application\\
				e = And the programme has been implemented \\
				a = 0-1 1-2 2-3 3-4 4-5 5-5 5-5
	\end{itemize}

\end{frame}

\begin{frame}{IBM2 modell}
	\begin{figure}[t]
	  	\includegraphics[width=1\linewidth]{images/ibm2}
	  	\caption{IBM2 modell\footnotemark}
	  \end{figure}
\footnotetext{\href{https://en.wikipedia.org/wiki/IBM_alignment_models}{Forrás IBM2 modell}}
\end{frame}